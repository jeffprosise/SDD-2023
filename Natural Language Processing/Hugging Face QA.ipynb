{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Question-Answering System with Hugging Face Transformers\n",
    "\n",
    "Hugging Face’s transformers package contains several pretrained BERT models already fine-tuned for specific tasks. One example is the “minilm-uncased-squad2” model, which was trained with Stanford’s [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset to answer questions by extracting text from documents. To get a feel for what models like this one can accomplish, let’s use it to build a simple question-answering system.\n",
    "\n",
    "Begin by using the following statements to load a pretrained MiniLM model from the Hugging-Face hub and a tokenizer to tokenize text input to the model. Then compose a pipeline from them. The first time you run this code, you'll experience a momentary delay while the pretrained weights are downloaded. After that, loading will be fast because the weights are cached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "id = 'deepset/minilm-uncased-squad2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(id)\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(id, from_pt=True)\n",
    "pipe = pipeline('question-answering', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face stores weights for this particular model in PyTorch format. The `from_pt=True` parameter converts the weights to TensorFlow format. It's not trivial to convert neural-network weights from one format to another, but the Hugging-Face library reduces it to a simple function parameter.\n",
    "\n",
    "Now use the pipeline to answer a question by extracting text from a paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9793193340301514,\n",
       " 'start': 0,\n",
       " 'end': 27,\n",
       " 'answer': 'Natural Language Processing'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'What does NLP stand for?'\n",
    "\n",
    "context = 'Natural Language Processing, or NLP, encompasses a variety of \\\n",
    "           activities, including text classification, keyword and topic \\\n",
    "           extraction, text summarization, and language translation. The \\\n",
    "           accuracy of NLP models has improved in recent years for a variety \\\n",
    "           of reasons, not the least of which are newer and better ways of \\\n",
    "           converting words and sentences into dense vector representations \\\n",
    "           that incorporate context, and a relatively new neural-network \\\n",
    "           architecture called the transformer that can zero in on the most \\\n",
    "           meaningful words and even differentiate between multiple meanings \\\n",
    "           of the same word.'\n",
    "\n",
    "pipe(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the answer accurate? A human could easily read the paragraph and come up with the same answer, but the fact that a deep-learning model can do it indicates that the model displays some level of reading comprehension. Observe that the output contains the answer to the question as well as a confidence score and the starting and ending indexes of the answer in the paragraph.\n",
    "\n",
    "Now try it again with a different question and context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'When was TensorFlow released?'\n",
    "\n",
    "context = 'Machine learning isn\\'t hard when you have a properly engineered \\\n",
    "           dataset to work with. The reason it\\'s not hard is libraries such as \\\n",
    "           Scikit-learn and ML.NET, which reduce complex learning algorithms to \\\n",
    "           a few lines of code. Deep learning isn’t difficult, either, thanks to \\\n",
    "           libraries such as the Microsoft Cognitive Toolkit (CNTK), Theano, \\\n",
    "           and PyTorch. But the library that most of the world has settled on \\\n",
    "           for building neural networks is TensorFlow, an open-source framework \\\n",
    "           created by Google that was released under the Apache License 2.0 in \\\n",
    "           2015.'\n",
    "\n",
    "pipe(question=question, context=context)['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this process with another question and context from which to extract an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'integrated into TensorFlow in 2019'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'Is Keras part of TensorFlow?'\n",
    "\n",
    "context = 'The learning curve for TensorFlow is rather steep. Another library \\\n",
    "           named Keras provides a simplified Python interface to TensorFlow \\\n",
    "           and has emerged as the Scikit of deep learning. Keras is all about \\\n",
    "           neural networks. It began life as a stand-alone project in 2015 \\\n",
    "           but was integrated into TensorFlow in 2019. Any code that you write \\\n",
    "           using TensorFlow’s built-in Keras module ultimately executes in \\\n",
    "           (and is optimized for) TensorFlow. Even Google recommends using the \\\n",
    "           Keras API.'\n",
    "\n",
    "pipe(question=question, context=context)['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform one final test using the same context as before but a different question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keras'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'Is it better to use Keras or TensorFlow to build neural networks?'\n",
    "pipe(question=question, context=context)['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The questions posed here were hand-selected to highlight the model's capabilities. It's not difficult to come up with questions that the model can't answer. Nevertheless, you have proved the principle that a pretrained BERT model fine-tuned on SQuAD 2.0 can answer straightforward questions from passages of text presented to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a retriever\n",
    "\n",
    "The reader's job is to extract answers from text containing answers. An equally important component of a QA pipeline is a retriever that selectively fetches text from a database or other document store and presents it to the reader. In real life, companies use tools such as [Haystack](https://haystack.deepset.ai/overview/intro) to build production-ready QA pipelines. Haystack provides highly scalable retrievers that operate against a variety of document stores, and it allows Hugging Face transformers such as MiniLM to serve as readers.\n",
    "\n",
    "Let's implement a simple retriever using a pretrained transformer model that converts paragraphs of text into vectors of floating-point numbers. We'll judge which paragraphs of text might contain answers by using dot products to measure the similarity between a vectorized question and vectorized versions of each paragraphs in a corpus of text. Begin by loading the dataset, which contains 25 paragraphs of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning isn’t hard when you have a properly engineered dataset to work with. The reason it’s not hard is libraries such as Scikit-learn and ML.NET, which reduce complex learning algorithms to a few lines of code. Deep learning isn’t difficult, either, thanks to libraries such as the Microsoft Cognitive Toolkit (CNTK), Theano, and PyTorch. But the library that most of the world has settled on for building neural networks is TensorFlow, an open-source framework created by Google that was released under the Apache License 2.0 in 2015.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TensorFlow isn’t limited to building neural networks. It is a framework for performing fast mathematical operations at scale using tensors, which are generalized versions of arrays. Tensors can represent scalar values (0-dimensional tensors), vectors (1D tensors), matrices (2D tensors), and so on. A neural network is basically a workflow for transforming tensors. The 3-layer perceptron featured in the previous chapter takes a 1D tensor containing two values as input, transforms it into a 1D tensor containing three values, and produces a 0D tensor as output. TensorFlow lets you define directed graphs that in turn define how tensors are computed. And unlike Scikit, it supports GPUs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The learning curve for TensorFlow is rather steep. Another library named Keras provides a simplified Python interface to TensorFlow and has emerged as the Scikit of deep learning. Keras is all about neural networks. It began life as a stand-alone project in 2015 but was integrated into TensorFlow in 2019. Any code that you write using TensorFlow’s built-in Keras module ultimately executes in (and is optimized for) TensorFlow. Even Google recommends using the Keras API.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Keras offers two APIs for building neural networks: a sequential API and a functional API. The former is simpler and is sufficient for most neural networks. The latter is useful in more advanced scenarios such as networks with multiple outputs – for example, a classification output and a regression output, which is common in neural networks that perform object detection – or shared layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s not difficult to use Scikit-learn to build machine-learning models that analyze text for sentiment, identify spam, and classify text in other ways. But today, state-of-the-art text classification is most often performed with neural networks. You already know how to build neural networks that accept numbers and images as input. Let’s build on that to learn how to construct deep-learning models that process text – a segment of deep learning known as natural language processing, or NLP for short.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   0\n",
       "0                                                                                                                                                 Machine learning isn’t hard when you have a properly engineered dataset to work with. The reason it’s not hard is libraries such as Scikit-learn and ML.NET, which reduce complex learning algorithms to a few lines of code. Deep learning isn’t difficult, either, thanks to libraries such as the Microsoft Cognitive Toolkit (CNTK), Theano, and PyTorch. But the library that most of the world has settled on for building neural networks is TensorFlow, an open-source framework created by Google that was released under the Apache License 2.0 in 2015.\n",
       "1  TensorFlow isn’t limited to building neural networks. It is a framework for performing fast mathematical operations at scale using tensors, which are generalized versions of arrays. Tensors can represent scalar values (0-dimensional tensors), vectors (1D tensors), matrices (2D tensors), and so on. A neural network is basically a workflow for transforming tensors. The 3-layer perceptron featured in the previous chapter takes a 1D tensor containing two values as input, transforms it into a 1D tensor containing three values, and produces a 0D tensor as output. TensorFlow lets you define directed graphs that in turn define how tensors are computed. And unlike Scikit, it supports GPUs.\n",
       "2                                                                                                                                                                                                                          The learning curve for TensorFlow is rather steep. Another library named Keras provides a simplified Python interface to TensorFlow and has emerged as the Scikit of deep learning. Keras is all about neural networks. It began life as a stand-alone project in 2015 but was integrated into TensorFlow in 2019. Any code that you write using TensorFlow’s built-in Keras module ultimately executes in (and is optimized for) TensorFlow. Even Google recommends using the Keras API.\n",
       "3                                                                                                                                                                                                                                                                                                           Keras offers two APIs for building neural networks: a sequential API and a functional API. The former is simpler and is sufficient for most neural networks. The latter is useful in more advanced scenarios such as networks with multiple outputs – for example, a classification output and a regression output, which is common in neural networks that perform object detection – or shared layers.\n",
       "4                                                                                                                                                                                            It’s not difficult to use Scikit-learn to build machine-learning models that analyze text for sentiment, identify spam, and classify text in other ways. But today, state-of-the-art text classification is most often performed with neural networks. You already know how to build neural networks that accept numbers and images as input. Let’s build on that to learn how to construct deep-learning models that process text – a segment of deep learning known as natural language processing, or NLP for short."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Data/passages.csv', header=None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load a pretrained model (and a tokenizer to go with it) that is adept at vectorizing text in such a way that the similarity between two vectorized samples can be quantified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertModel.\n",
      "\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "bert_id = 'sebastian-hofstaetter/distilbert-dot-margin_mse-T2-msmarco'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_id) \n",
    "bert_model = TFAutoModel.from_pretrained(bert_id, from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that uses the model to vectorize each of the paragraphs in the dataset. Then use the function to convert each paragraph into a vector (array) of 768 floating-point values and store the vectors in a variable named `vectorized_contexts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    tokenized_text = bert_tokenizer(text, return_tensors='tf')\n",
    "    vectorized_text = bert_model(tokenized_text)[0][:, 0, :][0]\n",
    "    return vectorized_text\n",
    "\n",
    "contexts = data[0]\n",
    "vectorized_contexts = contexts.apply(vectorize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that vectorizes a question and computes its similarity to each of the paragraphs stored in `vectorized_contexts`. Similarity is computed by taking the dot product of the question and each sample. The higher the dot product, the greater the similarity. `get_best_contexts` sorts the similarity scores in descending order and returns the indexes of the top `max_matches` results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_best_contexts(query, contexts, max_matches=3):\n",
    "    scores = pd.Series(dtype='object')\n",
    "    tokenized_query = bert_tokenizer(query, return_tensors='tf')\n",
    "    vectorized_query = bert_model(tokenized_query)[0][:, 0, :][0]\n",
    "    \n",
    "    for idx, item in contexts.iteritems():\n",
    "        score = np.dot(vectorized_query, item)\n",
    "        scores = pd.concat([scores, pd.Series(score)], ignore_index=True)\n",
    "\n",
    "    sorted_scores = scores.sort_values(ascending=False)[:max_matches]\n",
    "    return list(sorted_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `get_best_contexts` to identify the three paragraphs that are most likely to contain an answer to the question \"How many versions of YOLO are there?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are currently seven versions of YOLO referred to as YOLOv1 through YOLOv7. Each new version improves on the previous version in terms of accuracy and performance. There are also variations such as PP-YOLO and YOLO9000. YOLOv3 was the last version that YOLO creator Joseph Redmon contributed to and is considered a reference implementation of sorts. By extracting feature maps from certain layers of the CNN, YOLOv3 analyzes the image using a 13x13 grid, a 26x26 grid, and a 52x52 grid in an effort to detect objects of various sizes. It uses anchors to predict nine bounding boxes per cell. YOLO’s primary weakness is that it has difficulty detecting very small objects that are close together, although YOLOv3 improved on YOLOv1 and YOLOv2 in this regard. More information about YOLO can be found on its creator’s Web site. A separate article entitled “Digging deep into YOLO V3” offers a deep dive into the YOLOv3 architecture.\n",
      "\n",
      "While the R-CNN family of object-detection systems delivers unparalleled accuracy, it leaves something to be desired when it comes to real-time object detection of the type required by, say, self-driving cars. A paper entitled “You Only Look Once: Unified, Real-Time Object Detection” published in 2015 proposed an alternative to R-CNNs known as YOLO that revolutionized the way data scientists think about object detection. \n",
      "\n",
      "Until a few short years ago, most NMT models, including the one underlying Google Translate, were LSTM-based sequence-to-sequence models. In such models, one or more LSTM layers encode a tokenized input sequence representing the phrase to be translated into a vector. A second set of recurrent layers uses that vector as input and decodes it into a tokenized phrase in another language. The model accepts sequences as input and returns sequences as output, hence the term sequence-to-sequence model. A softmax output layer at the end outputs a set of probabilities for each token in the output sequence. If the maximum output phrase length that’s supported is 20 tokens, for example, and the vocabulary of the output language contains 20,000 words, then the output is 20 sets (one per token) of 20,000 probabilities. For each possible output token, the word selected is the word assigned the highest probability.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = 'How many versions of YOLO are there?'\n",
    "indexes = get_best_contexts(question, vectorized_contexts)\n",
    "\n",
    "for idx in indexes:\n",
    "    print(f'{contexts[idx]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the three paragraphs identified by `get_best_contexts` to the QA model to extract up to three answers. Show the answers, but skip empty answers — ones for which the starting and ending indexes are identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6229731440544128, 'start': 20, 'end': 25, 'answer': 'seven'}\n"
     ]
    }
   ],
   "source": [
    "for idx in indexes:\n",
    "    output = pipe(question=question, context=contexts[idx], handle_impossible_answer=True)\n",
    "    \n",
    "    if output['start'] != output['end']:\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that takes a question, a set of paragraphs, and the vectorized versions of those paragraphs as inputs and displays the answer or answers to the question. Then ask a question and show the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask R-CNNs (84.5%)\n"
     ]
    }
   ],
   "source": [
    "def show_answers(question, contexts, vectorized_contexts):\n",
    "    indexes = get_best_contexts(question, vectorized_contexts)\n",
    "    \n",
    "    for idx in indexes:\n",
    "        output = pipe(question=question, context=contexts[idx], handle_impossible_answer=True)\n",
    "\n",
    "        if output['start'] != output['end']:\n",
    "            print(f'{output[\"answer\"]} ({output[\"score\"]:.1%})')\n",
    "\n",
    "question = 'What type of neural network supports instance segmentation?'\n",
    "show_answers(question, contexts, vectorized_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it again, this time with a different question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it has difficulty detecting very small objects that are close together (47.0%)\n"
     ]
    }
   ],
   "source": [
    "question = 'What is YOLO\\'s primary weakness?'\n",
    "show_answers(question, contexts, vectorized_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a third question and see if the model is able to come up with an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow (13.0%)\n",
      "The learning curve for TensorFlow is rather steep (26.9%)\n"
     ]
    }
   ],
   "source": [
    "question = 'Is TensorFlow difficult to learn?'\n",
    "show_answers(question, contexts, vectorized_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a robust QA system that can answer the bulk of the questions presented to it on a particular topic obviously requires a much larger corpus of text than the 25 passages you used here. And the larger the corpus of text, the more such a system would benefit from a product such as Haystack. The key, however, is a reading-comprehension model that can make sense of the questions asked of it. For that, pretrained models from Hugging Face stand ready to get you up and running quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
