"Machine learning isn’t hard when you have a properly engineered dataset to work with. The reason it’s not hard is libraries such as Scikit-learn and ML.NET, which reduce complex learning algorithms to a few lines of code. Deep learning isn’t difficult, either, thanks to libraries such as the Microsoft Cognitive Toolkit (CNTK), Theano, and PyTorch. But the library that most of the world has settled on for building neural networks is TensorFlow, an open-source framework created by Google that was released under the Apache License 2.0 in 2015."
"TensorFlow isn’t limited to building neural networks. It is a framework for performing fast mathematical operations at scale using tensors, which are generalized versions of arrays. Tensors can represent scalar values (0-dimensional tensors), vectors (1D tensors), matrices (2D tensors), and so on. A neural network is basically a workflow for transforming tensors. The 3-layer perceptron featured in the previous chapter takes a 1D tensor containing two values as input, transforms it into a 1D tensor containing three values, and produces a 0D tensor as output. TensorFlow lets you define directed graphs that in turn define how tensors are computed. And unlike Scikit, it supports GPUs."
The learning curve for TensorFlow is rather steep. Another library named Keras provides a simplified Python interface to TensorFlow and has emerged as the Scikit of deep learning. Keras is all about neural networks. It began life as a stand-alone project in 2015 but was integrated into TensorFlow in 2019. Any code that you write using TensorFlow’s built-in Keras module ultimately executes in (and is optimized for) TensorFlow. Even Google recommends using the Keras API.
"Keras offers two APIs for building neural networks: a sequential API and a functional API. The former is simpler and is sufficient for most neural networks. The latter is useful in more advanced scenarios such as networks with multiple outputs – for example, a classification output and a regression output, which is common in neural networks that perform object detection – or shared layers."
"It’s not difficult to use Scikit-learn to build machine-learning models that analyze text for sentiment, identify spam, and classify text in other ways. But today, state-of-the-art text classification is most often performed with neural networks. You already know how to build neural networks that accept numbers and images as input. Let’s build on that to learn how to construct deep-learning models that process text – a segment of deep learning known as natural language processing, or NLP for short."
"NLP encompasses a variety of activities, including text classification, keyword and topic extraction, text summarization, and language translation. The accuracy of NLP models has improved in recent years for a variety of reasons, not the least of which are newer and better ways of converting words and sentences into dense vector representations that incorporate word context, and a relatively new neural-network architecture called the transformer that can zero in on the most meaningful words and even differentiate between different meanings of the same word."
"One element that virtually all neural networks that process text have in common is an embedding layer, which use word embeddings to transform arrays, or sequences, of scalar values representing words into arrays of word vectors. These vectors encode information about the meaning of words and relationships between them. Output from an embedding layer can be “flattened“ and input to a classification layer. Or the word vectors can be input directly to a recurrent layer or convolution layer to tease more meaning from them before doing any further processing."
"Text must be cleaned and vectorized before it’s used to train a neural network, too, but vectorization is typically performed in a different way. Rather than create a table of word counts, you create a table of sequences containing tokens representing individual words. Tokens are often indices into a dictionary, or vocabulary, built from the corpus of words in the dataset. To help, Keras provides the Tokenizer class, which you can think of as the deep-learning equivalent of CountVectorizer. Here’s an example that uses Tokenizer to create sequences from four lines of text:"
"The first hidden layer in a neural network that processes text is an embedding layer whose job is to convert padded sequences of word indices into arrays of word vectors, which represent each word with an array (vector) of floating-point numbers rather than a single integer. Each word in the input text is represented by a vector in the embedding layer, and as the network is trained, vectors representing individual words are adjusted to reflect their relationship to one another. If you’re building a sentiment-analysis model and words such as “excellent” and “amazing” have similar connotations, then the vectors representing those words in the embedding space should be relatively close together so phrases such as “excellent service” and “amazing service” score similarly."
"The vectors that represent individual words in an embedding layer are learned during training, just as the weights connecting neurons in adjacent dense layers are learned. If the number of training samples is sufficiently high, training the network usually creates effective vector representations of all the words. However, if you only have a few hundred training samples, the embedding layer might not have enough information to properly vectorize the corpus of text."
"Yet another way to factor word position into a classifier is to include recurrent layers in the network. Recurrent layers were originally invented to process time-series data – for example, to look at weather data for the last five days and predict what tomorrow’s high temperature will be. If you simply took all the weather data for the last five days and flattened it into a 1D array, any trends evident in the data would be lost. A recurrent layer, however, might detect those trends and factor them into its output. In a sense, a sequence of vectors output by an embedding layer is a time series because words in a phrase are ordered consecutively and words used early in a phrase could inform how words that occur later are interpreted."
"LSTM cells are miniature neural networks in their own right. As LSTM cells loop over the words in a sequence, they learn (just as the weights connecting neurons in dense layers are learned) which words are important and lend that information precedence in subsequent iterations of the loop. A dense layer doesn’t recognize that there’s a connection between “blue” and “sky” in the phrase “I like blue, for on a clear and sunny day, it is the color of the sky.” An LSTM layer does – information gleaned from the word “blue” carries all the way through to “sky” – and can factor that into predictions made by the network."
"Until a few short years ago, most NMT models, including the one underlying Google Translate, were LSTM-based sequence-to-sequence models. In such models, one or more LSTM layers encode a tokenized input sequence representing the phrase to be translated into a vector. A second set of recurrent layers uses that vector as input and decodes it into a tokenized phrase in another language. The model accepts sequences as input and returns sequences as output, hence the term sequence-to-sequence model. A softmax output layer at the end outputs a set of probabilities for each token in the output sequence. If the maximum output phrase length that’s supported is 20 tokens, for example, and the vocabulary of the output language contains 20,000 words, then the output is 20 sets (one per token) of 20,000 probabilities. For each possible output token, the word selected is the word assigned the highest probability."
"A landmark 2017 paper entitled “Attention Is All You Need” changed the way data scientists approach NMT and other neural text-processing tasks. It proposed a better way to perform sequence-to-sequence processing based on transformer models that eschew recurrent layers and use attention mechanisms to model the context in which words are used. Today, transformer models have almost entirely replaced LSTM-based encoders-decoders for translating text."
"The chief innovation introduced by the transformer model is the use of multi-head attention (MHA) layers in place of LSTM layers. MHA layers embody the concept of self-attention, which enables a model to analyze an input sequence and focus on the words that are most important as well as the context in which the words are used. In the sentence “We took a walk in the park,” for example, the word “park” has a different meaning than it does in “Where did you park the car?” An embedding layer stores one vector representation for “park,” but in a transformer model, the MHA layer modifies the vector output by the embedding layer so that “park” is represented by two different vectors in the two sentences."
"One way to apply deep learning to the task of object detection is to use region-based CNNs, also known as region CNNs or simply R-CNNs. The first R-CNN was introduced in a 2014 paper entitled “Rich feature hierarchies for accurate object detection and semantic segmentation.” The model described in the paper comprises three stages. The first stage scans the image and identifies up to 2,000 bounding boxes representing regions of interest – regions that might contain objects. The second stage is a deep CNN that extracts features from regions of interest. The third is a support-vector machine (or series of SVMs – one per class) that classifies the features. The output is a collection of bounding boxes with class labels and confidence scores. An algorithm called non-maximum suppression (NMS) filters the output and selects the best bounding box for each object."
"NMS is a crucial element of virtually all modern object-detection systems. A detector invariably emits several bounding boxes for each object. If a photo contains one instance of a given class – for example, one zebra – NMS selects the bounding box with the highest confidence score. If the photo contains two zebras (Figure 12-2), NMS divides the bounding boxes into two groups and selects the box with the highest confidence score in each group. It groups boxes based on the amount of overlap between them. Overlap is computed by dividing the area of intersection between two boxes by the area formed by the union of the boxes. If the resulting intersection-over-union (IoU) score is greater than a predetermined threshold (typically 0.5), NMS assigns the boxes to the same group. Otherwise, it assigns them to separate groups."
"A 2015 paper entitled “Fast R-CNN” addressed this by proposing a modified architecture in which the entire image passes through the CNN one time. Selective search or a similar algorithm identifies regions of interest in the image, and those regions are projected onto the feature map generated by the CNN. An ROI pooling layer then uses a form of max-pooling to reduce the features in each region of interest to a fixed-length vector, independent of the region’s size and shape. (By contrast, R-CNN scales each region to an image of predetermined size before submitting it to the CNN, which is substantially more expensive than ROI pooling.) Classification of the feature vectors is performed by fully connected layers rather than SVMs, and the output is split to include both a softmax classifier and a bounding-box regressor. NMS filters the bounding boxes down to the ones that matter. The result is a system that trains an order of magnitude father than R-CNN, makes predictions two orders of magnitude faster, and is slightly more accurate than R-CNN."
"A 2016 paper entitled “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks” further boosted performance by replacing selective search with a region-proposal network, or RPN. The RPN is a shallow CNN that shares layers with the main CNN (Figure 12-5). To generate region proposals, it slides a window over the feature map generated by the last shared layer. At each stop in the window's travel, the RPN evaluates n candidate regions called anchors and computes an objectness score for each anchor based on IoU scores with ground-truth boxes. Objectness scores and anchor boxes are input to fully connected layers for classification (does the anchor contain an object, or does it not?) and regression. The output from these layers ultimately determines the regions of interest projected onto the feature map generated by the main CNN and forwarded to the ROI pooling layer."
"The next chapter in the R-CNN story came in 2017 in a paper entitled “Mask R-CNN.” Mask R-CNNs extend Faster R-CNNs by adding instance segmentation, which identifies the shapes of objects detected in an image using segmentation masks like the ones in Figure 12-6. Performance impact is minimal because instance segmentation is performed in parallel with region evaluation. The benefit of Mask R-CNNs is that they provide more detail about the objects they detect. For example, you can tell whether a person’s arms are extended or whether that person is standing up or lying down – something you can’t discern from a simple bounding box. They are also slightly more accurate than Faster R-CNNs because they replace ROI pooling with ROI alignment, which discards less information when generating feature vectors whose boundaries don’t perfectly align with the boundaries of the regions they represent."
"While the R-CNN family of object-detection systems delivers unparalleled accuracy, it leaves something to be desired when it comes to real-time object detection of the type required by, say, self-driving cars. A paper entitled “You Only Look Once: Unified, Real-Time Object Detection” published in 2015 proposed an alternative to R-CNNs known as YOLO that revolutionized the way data scientists think about object detection. "
"There are currently seven versions of YOLO referred to as YOLOv1 through YOLOv7. Each new version improves on the previous version in terms of accuracy and performance. There are also variations such as PP-YOLO and YOLO9000. YOLOv3 was the last version that YOLO creator Joseph Redmon contributed to and is considered a reference implementation of sorts. By extracting feature maps from certain layers of the CNN, YOLOv3 analyzes the image using a 13x13 grid, a 26x26 grid, and a 52x52 grid in an effort to detect objects of various sizes. It uses anchors to predict nine bounding boxes per cell. YOLO’s primary weakness is that it has difficulty detecting very small objects that are close together, although YOLOv3 improved on YOLOv1 and YOLOv2 in this regard. More information about YOLO can be found on its creator’s Web site. A separate article entitled “Digging deep into YOLO V3” offers a deep dive into the YOLOv3 architecture."
"One of the fastest and most popular algorithms for detecting faces in photos stems from a paper published in 2001 entitled “Rapid Object Detection using a Boosted Cascade of Simple Features.” Sometimes known as Viola-Jones (the authors of the paper), the algorithm keys on the relative intensities of adjacent blocks of pixels. For example, the average pixel intensity in a rectangle around the eyes is typically darker than the average pixel intensity in a rectangle immediately below. Similarly, the bridge of the nose is usually lighter than the region around the eyes, so two dark rectangles with a bright rectangle in the middle might represent two eyes and a nose. The presence of many such Haar-like features in a frame at the right locations is an indicator that the frame contains a face."
"The key to Viola-Jones’ performance is the binary classifier. A frame that is 24 pixels wide and 24 pixels high contains more than 160,000 combinations of rectangles representing potential Haar-like features. Rather than compute values for every combination, Viola-Jones computes only those that the classifier requires. Furthermore, how many features the classifier requires depends on the content of the frame. The classifier is actually several binary classifiers arranged in stages. The first stage might require just one feature. The second stage might require 10, the third might require 20, and so on. Features are only extracted and passed to stage n if stage n-1 returns positive, giving rise to the term cascade classifier."
"While more computationally expensive, deep-learning methods often do a better job of detecting faces in images than Viola-Jones. In particular, multitask cascaded convolutional neural networks, or MTCNNs, have proven adept at face detection in a variety of benchmarks. They also identify facial landmarks such as the eyes, the nose, and the mouth."
