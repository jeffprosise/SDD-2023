{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "[LangChain](https://python.langchain.com/en/latest/index.html) is an open-source framework for developing apps powered by Large Language Models (LLMs) such as [ChatGPT](https://openai.com/blog/chatgpt). With a few lines of code, you can build apps that answer questions from documents, databases, and more. The following example uses LangChain to \"chunk\" Microsoft's 2022 annual report (a 90-page Microsoft Word document) and ChatGPT to answer questions from the report. In order to run this notebook, you must store your OpenAI API key in an environment variable named `OPENAI_API_KEY`.\n",
    "\n",
    "![](Images/langchain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by using LangChain's loader for DOCX files to chunk Microsoft's 2022 annual report and create a vector database from the chunks. By default, LangChain uses OpenAI's [Embeddings API](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) (`text-embedding-ada-002`) to create vectors from chunks of text, and it stores the vectors in an in-memory database provided by [DuckDB](https://duckdb.org/). This example persists the vector database in the \"Data\" subdirectory so the database can easily be recreated later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: Data\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# Load the annual report\n",
    "loader = Docx2txtLoader('Data/annual-report.docx')\n",
    "\n",
    "# Create an index over it using an in-memory vector store\n",
    "index = VectorstoreIndexCreator(vectorstore_kwargs={ 'persist_directory': 'Data'}).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the annual report is chunked and vectorized, asking a question is a simple matter of calling the `query` method of the `VectorstoreIndexWrapper` object in `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, Microsoft repurchased 95 million shares in 2022, with a total value of $28.033 billion.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a query on the annual report\n",
    "index.query('Did Microsoft repurchase shares in 2022, and if so, how many shares did it repurchase and what was the value of those shares?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you don't want to have to chunk and vectorize a document or set of documents each time an app starts up. The next example demonstrates how to recreate the vector store from the files persisted in the \"Data\" subdirectory. Currently, this requires adding a method to the `VectorstoreIndexCreator` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "# Add a from_persistent_index method to VectorstoreIndexCreator\n",
    "def from_persistent_index(self, path: str)-> VectorStoreIndexWrapper:\n",
    "    vectorstore = self.vectorstore_cls(persist_directory=path, embedding_function=self.embedding)\n",
    "    return VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
    "\n",
    "VectorstoreIndexCreator.from_persistent_index=from_persistent_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `from_persistent_index` method can now be used to recreate the vector store and ask questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: Data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 32%'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator().from_persistent_index('Data')\n",
    "\n",
    "index.query('What was Microsoft\\'s year-over-year increase in cloud revenue from 2021 to 2022?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer to the question above can be gleaned from page 83 of the annual report. Did the LLM get it right?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
