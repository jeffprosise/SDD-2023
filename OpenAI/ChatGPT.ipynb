{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT\n",
    "\n",
    "[ChatGPT](https://openai.com/blog/chatgpt) is one of several sophisticated AI models built by [OpenAI](https://openai.com/), a San Francisco-based company whose mission is to \"ensure that artificial general intelligence benefits all of humanity.\" ChatGPT is a Large Language Model (LLM) built on GPT-3.5, which boasts 175 billion parameters and ranks as the largest neural network ever built. ChatGPT can generate human-like prose by responding to instructions written in the [Chat Markup Language](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt), or ChatML for short. Here are few examples demonstrating how to leverage ChatGPT using its REST API.\n",
    "\n",
    "![](Images/chatgpt.jpg)\n",
    "\n",
    "Before running this notebook, create an environment variable named `API_KEY` and set it equal to your OpenAI API key. If you don't already have an OpenAI account and an API key to go with it, go to https://platform.openai.com/signup and create an account. Then go to the API Keys page, generate an API key, and save it where you can easily retrieve it later. OpenAI grants you a certain amount of free usage each month, so you don't have to create a paid account in order to get an API key. Once generated, an OpenAI API key can't be retrieved by returning to the OpenAI Web site. Be sure to save it in a secure place so you can retrieve it when needed. If you lose an API key, your only recourse is to generate a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai\n",
    "\n",
    "openai.api_key = os.environ['API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask ChatGPT to write a paragraph of text describing molecular biology in the style of Dr. Seuss. Run this cell several times and you'll get a different result each time. Set `temperature` to 0.0, however, and the results will be the same most of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world of cells so small and neat,\n",
      "Molecular biology is the ultimate feat!\n",
      "It's the study of molecules so tiny and bright,\n",
      "And how they work to make life so right.\n",
      "\n",
      "In every cell, a dance occurs,\n",
      "As proteins and RNA twist and turn.\n",
      "They make enzymes, they make machines,\n",
      "And carry out life's biological schemes.\n",
      "\n",
      "DNA, the master plan,\n",
      "Controls it all with a stern hand.\n",
      "It guides the building blocks of life,\n",
      "And helps cells grow and end strife.\n",
      "\n",
      "In molecular biology, we explore,\n",
      "The secrets of cells to the core.\n",
      "We learn how they work and how they thrive,\n",
      "And discover new ways to keep life alive.\n",
      "\n",
      "So let's take a look inside,\n",
      "At molecules so small they often hide.\n",
      "Through molecular biology, we'll uncover,\n",
      "The secrets of life, forever and ever!\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    'role': 'user',\n",
    "    'content': 'Describe molecular biology in the style of Dr. Seuss'\n",
    "}]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can richen the UI experience by streaming the response. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, let me tell you 'bout a thing called molecular biology,\n",
      "It's all about the molecules, and how they like to party!\n",
      "\n",
      "There's DNA and RNA, they're really quite a team,\n",
      "They hold the information to create life's most fantastic schemes.\n",
      "\n",
      "A-T and C-G, those are their little letters,\n",
      "They spell out all the secrets that make life so much better.\n",
      "\n",
      "Proteins are the party animals, they like to boogie down,\n",
      "They come in all shapes and sizes, and they never wear a frown.\n",
      "\n",
      "Their job is to make things happen, to keep the cell alive,\n",
      "They're like the workers in a factory, always ready to thrive.\n",
      "\n",
      "There's enzymes, receptors, transporters too,\n",
      "All working together to make sure the cell pulls through.\n",
      "\n",
      "And let's not forget about the membrane, that's a real star,\n",
      "It's like the cell's protective shield, always up to par.\n",
      "\n",
      "So that's the basics of molecular biology,\n",
      "It's really quite fantastic, as you can plainly see.\n",
      "\n",
      "Now go out there and explore this wondrous world of science,\n",
      "Because with molecular biology, you're really in for some defiance!"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    'role': 'user',\n",
    "    'content': 'Describe molecular biology in the style of Dr. Seuss'\n",
    "}]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messages transmitted to ChatGPT use the Chat Markup Language. ChatML exists so that ChatGPT can be given instructions and so that context can be preserved across calls. To demonstrate, ask ChatGPT what its name is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I don't have a name, but you can call me OpenAI. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    'role': 'user',\n",
    "    'content': 'My name is Jeff. What\\'s your name?' \\\n",
    "}]\n",
    " \n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jeff! My name is Sam. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a friendly chatbot named Sam'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'My name is Jeff. What\\'s your name?'\n",
    "    }\n",
    "]\n",
    " \n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can be as specific as you’d like with `system` messages, even saying \"If you don’t know the answer to a question, say I don’t know.\" You can also prescribe a persona. Replace \"friendly\" with \"sarcastic\" in the message from system and run the code again. The response may be \"Oh, hi Jeff, I’m Sam. You can call me whatever you'd like, but don’t call me late for dinner.\" Run the code several times and there’s no end to the colorful responses you’ll receive.\n",
    "\n",
    "ChatML's greatest power lies in persisting context from one call to the next. As an example, try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Jeff, my name is Sam. I'm here to assist you today.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a friendly chatbot named Sam'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'My name is Jeff. What\\'s your name?'\n",
    "    }\n",
    "]\n",
    " \n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then follow up immediately with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I do not have access to your name. May I know what you would like to be called?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a friendly chatbot named Sam'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What is my name?'\n",
    "    }\n",
    "]\n",
    " \n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT will respond with something along the lines of \"I'm sorry, but I don’t have access to that information.\" But now try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Jeff, as you mentioned earlier. Is there anything else you would like to know, Jeff?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a friendly chatbot named Sam'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'My name is Jeff. What\\'s your name?'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': 'Hello Jeff, my name is Sam. Nice to meet you!'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What is my name?'\n",
    "    }\n",
    "]\n",
    " \n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get it? Calls to ChatGPT are stateless. If you give ChatGPT your name in one call and ask it to repeat your name in the next call, ChatGPT has no clue. But with ChatML, you can provide past ChatGPT responses as context for the current call. You could easily build a conversational bot simply by repeating the last few prompts and responses in each call to ChatGPT. The further back you go, the longer the chatbot’s \"memory\" will be."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "ChatGPT doesn't work with words; it works with *tokens*. Tokenization plays an important role in Natural Language Processing. Neural networks can’t process text, at least not directly; they only process numbers. Tokenization converts words into numbers that a deep-learning model can understand. When ChatGPT generates a response by predicting a series of tokens, the tokenization process is reversed to convert the tokens into human-readable text.\n",
    "\n",
    "ChatGPT uses a form of tokenization called [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (BPE), which was developed in the 1990s as a mechanism for compressing text. Today, it is widely used in the NLP space. Here’s how ChatGPT BPE-tokenizes the phrase \"fourscore and seven years ago:\"\n",
    "\n",
    "![](Images/bpe.png)\n",
    "\n",
    "As a rule of thumb, 3 words on average translate to about 4 BPE tokens. That’s important because ChatGPT limits the number of tokens in each API call. The maximum token count is controlled by a parameter named `max_tokens`. For ChatGPT, the default is 2,048 tokens or about 1,500 words, and the upper limit is 4,096. (GPT-4 expands the maximum token count to 32,768.) This limit applies to the combined length of the input and output in each API call. If the number of tokens exceeds `max_tokens`, then either the call will fail or the response will be truncated.\n",
    "\n",
    "You can compute the number of tokens generated from a text sample with help from a Python package named [`tiktoken`](https://pypi.org/project/tiktoken/0.3.0/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 tokens\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    " \n",
    "text = 'Jeff loves to build and fly model jets. He built his first ' \\\n",
    "       'jet, a BVM BobCat, in 2007. After that, he built a BVM Bandit, ' \\\n",
    "       'a Skymaster F-16, and a Skymaster F-5. The latter two are 1/6th' \\\n",
    "       'scale models of actual fighter jets. Top speed is around 200 MPH.'\n",
    " \n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "num_tokens = len(encoding.encode(text))\n",
    "print(f'{num_tokens} tokens')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can estimate the token count for an entire `messages` array with the following code, which was adapted comments and all from the [ChatGPT documentation](https://platform.openai.com/docs/guides/chat/introduction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 tokens\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 0\n",
    " \n",
    "for message in messages:\n",
    "    num_tokens += 4 # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "    for key, value in message.items():\n",
    "        num_tokens += len(encoding.encode(value))\n",
    "        if key == 'name':  # if there's a name, the role is omitted\n",
    "            num_tokens += -1 # role is always required and always 1 token\n",
    "             \n",
    "num_tokens += 2 # every reply is primed with <im_start>assistant\n",
    "print(f'{num_tokens} tokens')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of reasons to be aware of the token count in each call. First, you’re charged by the token for input and output. The price at the time of this writing was 0.2 cents per 1,000 tokens, which equates to 500,000 tokens – or roughly 375,000 words – per U.S. dollar. The larger the `messages` array and the longer the response, the more you pay. Second, when using the messages array to provide context from previous calls, you have a finite amount of space to work with. It's common practice to pick a number – say, 5 or 10 – and limit the context from previous calls to that number of messages, or to programmatically compute the number of tokens that a conversation comprises and include as many messages as `max_tokens` will allow while leaving room for the response."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text\n",
    "\n",
    "ChatGPT can perform many NLP tasks such as sentiment analysis and neural machine translation (NMT) without further training. Here's an example that translates text from English to French. It's a good idea to set `temperature` to 0 here since you generally want translations to be accurate and repeatable rather than creative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeff adore construire et piloter des jets modèles réduits. Il a construit son premier jet, un BVM BobCat, en 2007. Après cela, il a construit un BVM Bandit, un Skymaster F-16 et un Skymaster F-5. Ces deux derniers sont des modèles à l'échelle 1/6 de véritables avions de chasse. La vitesse maximale est d'environ 200 MPH.\n"
     ]
    }
   ],
   "source": [
    "content =  f'Translate the following text from English to French: {text}'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT understands dozens of languages. It even knows Klingon. Let's see how the same paragraph translates to Klingon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeff vIghro' 'ej vIghoS model jets. vIghro' BVM BobCat, 2007. DaH jImej, vIghoS BVM Bandit, Skymaster F-16, 'ej Skymaster F-5. cha'logh cha'logh 1/6th scale fighter jets. yIbuS 200 MPH.\n"
     ]
    }
   ],
   "source": [
    "content = f'Translate the following text from English to Klingon: {text}'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the practical uses for ChatGPT is editing existing text to make it more compelling. Here's an example that generates a marketing blurb for a webinar on AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discover the fascinating world of AI and uncover its powerful impact on the world of business! Peek behind the curtain to see how this innovative technology operates and how it's transforming industries as we know them. Gain a fresh perspective and equip yourself with the knowledge to stay ahead of the game in today's fast-paced world. Let's dive in and discover the limitless possibilities of AI together!\n"
     ]
    }
   ],
   "source": [
    "content = 'Make the following marketing text more engaging: ' \\\n",
    "          'Learn how AI works and how it\\'s used in business'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples demonstrate how to use ChatGPT for sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "content = 'Indicate whether the following review\\'s sentiment is positive or ' \\\n",
    "          'negative: Great food and excellent service'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "content = 'Indicate whether the following review\\'s sentiment is positive or ' \\\n",
    "          'negative: Long lines and poor customer service'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is a text-classification task. ChatGPT can classify text in other ways, too. The next two examples demonstrate how it could be used as the basis for a spam filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not spam.\n"
     ]
    }
   ],
   "source": [
    "content = 'Indicate whether the following email is spam or not spam: ' \\\n",
    "          'Please plan to attend the code review at 2:00 p.m. this afternoon'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "content = 'Indicate whether the following email is spam or not spam: ' \\\n",
    "          'Order prescription meds online and save $$$'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT was trained on a massive body of data that includes [Common Crawl](https://commoncrawl.org/), a pair of book databases known as Books1 and Books2, [WebText2](https://www.eleuther.ai/projects/owt2/), and [Wikipedia](https://www.wikipedia.org/), so it's knowledge is vast. It can generally answer questions as long as the answers are somewhere on the Internet. Its answers aren't always correct because (surprise!) there's a lot of misinformation on the Internet. Here's a question that's pertinent to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The deep learning transformer was introduced in the academic paper \"Attention Is All You Need\" by Vaswani et al. The paper was published in the Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS 2017). The paper can be accessed online from the official NIPS website or from the arXiv preprint server.\n"
     ]
    }
   ],
   "source": [
    "content = 'In which academic paper was the deep learning transformer ' \\\n",
    "          'introduced, and where can I find it?'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT can also answer questions from context that you provide. Here's an example that involves my hobby. Observe that ChatGPT seems to have the ability to count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeff has built four jets.\n"
     ]
    }
   ],
   "source": [
    "question = 'How many jets has Jeff built?'\n",
    "\n",
    "content = f'Answer the following question using the provided context, and if the ' \\\n",
    "          f'answer is not contained within the context, say \"I don\\'t know.\"\\n\\n' \\\n",
    "          f'Context: {text}\\n\\n' \\\n",
    "          f'Q: {question}\\n\\n' \\\n",
    "          f'A: '\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "    \n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT is frequently used to ingest call transcripts and call notes and answer questions for statements of work (SOWs) and scoping documents. Here's an example involving the transcript of a recent [PBS interview](https://www.pbs.org/thinktank/transcript1292.html) with Elon Musk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk is affiliated with multiple companies, including PayPal, Tesla Motors, and SpaceX. PayPal is an internet payment mechanism, while Tesla Motors focuses on creating electric cars. SpaceX is developing markets for taking satellites and people to orbit and beyond."
     ]
    }
   ],
   "source": [
    "with open('Data/musk.txt', 'r') as input_file:\n",
    "    text = input_file.read()\n",
    "    \n",
    "    question = 'What companies is Elon Musk affiliated with, and what do those companies do?'\n",
    "\n",
    "    content = f'Answer the following question using the provided context, and if the ' \\\n",
    "              f'answer is not contained within the context, say \"I don\\'t know.\"\\n\\n' \\\n",
    "              f'Context: {text}\\n\\n' \\\n",
    "              f'Q: {question}\\n\\n' \\\n",
    "              f'A: '\n",
    "\n",
    "    messages = [{ 'role': 'user', 'content': content }]\n",
    "    \n",
    "    chunks = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for chunk in chunks:\n",
    "        content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "        if content is not None:\n",
    "            print(content, end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to read the transcript and submit questions of your own to see how ChatGPT responds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with code\n",
    "\n",
    "In addition to being trained on massive volumes of text comprising approximately 500 billion words, ChatGPT was trained with billions of lines of code. It can generate code, comment code, find bugs in code, and more, and it supports dozens of programming languages. Let's use a few examples to see what it's capable of. Start by using ChatGPT to implement a bubble sort in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python function that accepts an array of numbers as input, bubble sorts the numbers, and returns a sorted array:\n",
      "\n",
      "```python\n",
      "def bubble_sort(arr):\n",
      "    n = len(arr)\n",
      "    for i in range(n):\n",
      "        for j in range(0, n-i-1):\n",
      "            if arr[j] > arr[j+1]:\n",
      "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "    return arr\n",
      "```\n",
      "\n",
      "To use this function, simply call it with an array of numbers as an argument:\n",
      "\n",
      "```python\n",
      "arr = [4, 2, 7, 1, 3]\n",
      "sorted_arr = bubble_sort(arr)\n",
      "print(sorted_arr) # [1, 2, 3, 4, 7]\n",
      "```\n",
      "\n",
      "The function works by iterating through the entire array multiple times, comparing adjacent elements and swapping them if they are out of order. The iteration stops when no more swaps are needed, indicating that the array is fully sorted."
     ]
    }
   ],
   "source": [
    "content = 'Create a Python function that accepts an array of numbers as ' \\\n",
    "          'input, bubble sorts the numbers, and returns a sorted array'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ask ChatGPT to explain the code that it just generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code implements the bubble sort algorithm to sort an array of numbers in ascending order. The function takes an array 'arr' as input and calculates its length 'n'. It then iterates 'n' number of times, swapping adjacent elements in the array if they are in the wrong order. After each iteration, the largest unsorted element is moved to the end of the array. The sorted array is returned as output."
     ]
    }
   ],
   "source": [
    "content = 'Explain what the following code does:\\n' \\\n",
    "          'def bubble_sort(arr):\\n' \\\n",
    "          '    n = len(arr)\\n' \\\n",
    "          '    for i in range(n):\\n' \\\n",
    "          '        for j in range(0, n-i-1):\\n' \\\n",
    "          '            if arr[j] > arr[j+1]:\\n' \\\n",
    "          '                arr[j], arr[j+1] = arr[j+1], arr[j]\\n' \\\n",
    "          '    return arr'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ChatGPT to convert a Python function to FORTRAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subroutine bubble_sort(arr, n)\n",
      "    integer n\n",
      "    real arr(n)\n",
      "    do i = 1, n\n",
      "        do j = 1, n-i\n",
      "            if (arr(j) > arr(j+1)) then\n",
      "                temp = arr(j)\n",
      "                arr(j) = arr(j+1)\n",
      "                arr(j+1) = temp\n",
      "            end if\n",
      "        end do\n",
      "    end do\n",
      "end subroutine"
     ]
    }
   ],
   "source": [
    "content = 'Convert the following Python function into a FORTRAN function:\\n' \\\n",
    "          'def bubble_sort(arr):\\n' \\\n",
    "          '    n = len(arr)\\n' \\\n",
    "          '    for i in range(n):\\n' \\\n",
    "          '        for j in range(0, n-i-1):\\n' \\\n",
    "          '            if arr[j] > arr[j+1]:\\n' \\\n",
    "          '                arr[j], arr[j+1] = arr[j+1], arr[j]\\n' \\\n",
    "          '    return arr'\n",
    "\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ChatGPT to add comments to a block of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define bubble sort function\n",
      "def bubble_sort(arr):\n",
      "    # Get length of array\n",
      "    n = len(arr)\n",
      "    # Loop through array to compare elements\n",
      "    for i in range(n):\n",
      "        # Inner loop for comparing adjacent elements\n",
      "        for j in range(0, n-i-1):\n",
      "            # Swap if element at j is greater than element at (j+1)\n",
      "            if arr[j] > arr[j+1]:\n",
      "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "    # Return the sorted array\n",
      "    return arr"
     ]
    }
   ],
   "source": [
    "content = 'Add inline comments to the following code:\\n' \\\n",
    "          'def bubble_sort(arr):\\n' \\\n",
    "          '    n = len(arr)\\n' \\\n",
    "          '    for i in range(n):\\n' \\\n",
    "          '        for j in range(0, n-i-1):\\n' \\\n",
    "          '            if arr[j] > arr[j+1]:\\n' \\\n",
    "          '                arr[j], arr[j+1] = arr[j+1], arr[j]\\n' \\\n",
    "          '    return arr'\n",
    "\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to write a tool to add comments to uncommented source-code files. The next example loads **app.py**, adds comments to it, and saves the results in **app-commented.py**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "with open('Data/app.py', 'r') as input_file:\n",
    "    lines = input_file.read()\n",
    "    content = 'Add inline comments to the following code:\\n' + lines\n",
    "\n",
    "    messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    with open('Data/app-commented.py', 'w') as output_file:\n",
    "        output_file.write(response.choices[0].message.content)\n",
    "        print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works with C# source code as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "with open('Data/Program.cs', 'r') as input_file:\n",
    "    lines = input_file.read()\n",
    "    content = 'Add inline comments to the following code:\\n' + lines\n",
    "\n",
    "    messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    with open('Data/Program-commented.cs', 'w') as output_file:\n",
    "        output_file.write(response.choices[0].message.content)\n",
    "        print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaking of C#: Can ChatGPT rewrite a block of C# code that manually iterates over a `List` to use LINQ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var picks = stocks.Where(stock => stock.Close > stock.Open).ToList();\n",
      "picks.ForEach(pick => Console.WriteLine($\"{pick.Symbol}: {pick.Open:c} -> {pick.Close:c}\"));\n"
     ]
    }
   ],
   "source": [
    "content = 'Rewrite the following C# code to use LINQ:\\n' \\\n",
    "          'var picks = new List<DailyStock>();\\n' \\\n",
    "          'foreach (var stock in stocks)\\n' \\\n",
    "          '{\\n' \\\n",
    "          '    if (stock.Close > stock.Open)\\n' \\\n",
    "          '    {\\n' \\\n",
    "          '        picks.Add(stock);\\n' \\\n",
    "          '    }\\n' \\\n",
    "          '}\\n' \\\n",
    "          'foreach(var pick in picks)\\n' \\\n",
    "          '{\\n' \\\n",
    "          '    Console.WriteLine($\"{pick.Symbol}: {pick.Open:c} -> {pick.Close:c}\");\\n' \\\n",
    "          '}\\n'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ChatGPT to generate a non-trivial SQL query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT d.name as department_name\n",
      "FROM Department d\n",
      "JOIN Employee e ON d.id = e.department_id\n",
      "JOIN Salary_Payments sp ON e.id = sp.employee_id\n",
      "WHERE sp.date >= DATEADD(month, -3, GETDATE())\n",
      "GROUP BY d.name\n",
      "HAVING COUNT(DISTINCT e.id) >= 10;"
     ]
    }
   ],
   "source": [
    "content = 'Generate a SQL query to list the names of all departments ' \\\n",
    "          'that have employed 10 or more people in the last 3 months. ' \\\n",
    "          'The query targets a table with the following schema:\\n' \\\n",
    "          'Employee(id, name, department_id)\\n' \\\n",
    "          'Department(id, name, address)\\n' \\\n",
    "          'Salary_Payments(id, employee_id, amount, date)\\n'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT is cognizant of popular programming libraries and frameworks, too. Here, it relies on one of them to generate a function that performs sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I can provide you with the following code snippet that uses the TextBlob library to analyze the sentiment of a given string:\n",
      "\n",
      "```\n",
      "from textblob import TextBlob\n",
      "\n",
      "def analyze_sentiment(text):\n",
      "    blob = TextBlob(text)\n",
      "    sentiment = blob.sentiment.polarity\n",
      "    normalized_sentiment = (sentiment + 1) / 2\n",
      "    return normalized_sentiment\n",
      "```\n",
      "\n",
      "The `analyze_sentiment` function takes a string as input and creates a `TextBlob` object from it. The `sentiment` property of the `TextBlob` object returns a value between -1.0 and 1.0, where negative values indicate negative sentiment and positive values indicate positive sentiment. \n",
      "\n",
      "To convert this value to a scale of 0.0 to 1.0, we add 1 to the sentiment value and divide by 2. This maps the sentiment value from the range of -1.0 to 1.0 to the range of 0.0 to 1.0.\n",
      "\n",
      "You can call this function with any string and it will return a sentiment score between 0.0 and 1.0."
     ]
    }
   ],
   "source": [
    "content = 'Generate a Python function that accepts a string as input and ' \\\n",
    "          'analyzes the string for sentiment. The function returns a value ' \\\n",
    "          'from 0.0 to 1.0, where 0.0 means the sentiment is very negative ' \\\n",
    "          'and 1.0 means it is very positive.'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the more remarkable aspects of ChatGPT is its ability to find bugs in code. Just after my book went to the printer in 2022, I discovered a bug in the latest version of Scikit-learn that prevented some of my samples from working properly. I spent a couple of hours in the source code and found the bug. I filed a [bug report](https://github.com/scikit-learn/scikit-learn/issues/24942) and the Scikit team confirmed the bug and promised to fix it in the next version. I had to scramble to rewrite some of the code samples in my book to work around the bug and get the changes to the printer before the presses started rolling. Let's see if ChatGPT can find the bug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bug is that the crop operation on the PIL Image object does not modify it in place. Therefore, the cropped image needs to be assigned back to pil_img before resizing it.\n",
      "\n",
      "Below is the fixed code:\n",
      "\n",
      "```\n",
      "def _load_imgs(file_paths, slice_, color, resize):\n",
      "    \"\"\"Internally used to load images\"\"\"\n",
      "    try:\n",
      "        from PIL import Image\n",
      "    except ImportError:\n",
      "        raise ImportError(\n",
      "            \"The Python Imaging Library (PIL) is required to load data \"\n",
      "            \"from jpeg files. Please refer to \"\n",
      "            \"https://pillow.readthedocs.io/en/stable/installation.html \"\n",
      "            \"for installing PIL.\"\n",
      "        )\n",
      "\n",
      "    # compute the portion of the images to load to respect the slice_ parameter\n",
      "    # given by the caller\n",
      "    default_slice = (slice(0, 250), slice(0, 250))\n",
      "    if slice_ is None:\n",
      "        slice_ = default_slice\n",
      "    else:\n",
      "        slice_ = tuple(s or ds for s, ds in zip(slice_, default_slice))\n",
      "\n",
      "    h_slice, w_slice = slice_\n",
      "    h = (h_slice.stop - h_slice.start) // (h_slice.step or 1)\n",
      "    w = (w_slice.stop - w_slice.start) // (w_slice.step or 1)\n",
      "\n",
      "    if resize is not None:\n",
      "        resize = float(resize)\n",
      "        h = int(resize * h)\n",
      "        w = int(resize * w)\n",
      "\n",
      "    # allocate some contiguous memory to host the decoded image slices\n",
      "    n_faces = len(file_paths)\n",
      "    if not color:\n",
      "        faces = np.zeros((n_faces, h, w), dtype=np.float32)\n",
      "    else:\n",
      "        faces = np.zeros((n_faces, h, w, 3), dtype=np.float32)\n",
      "\n",
      "    # iterate over the collected file path to load the jpeg files as numpy\n",
      "    # arrays\n",
      "    for i, file_path in enumerate(file_paths):\n",
      "        if i % 1000 == 0:\n",
      "            logger.debug(\"Loading face #%05d / %05d\", i + 1, n_faces)\n",
      "\n",
      "        # Checks if jpeg reading worked. Refer to issue #3594 for more\n",
      "        # details.\n",
      "        pil_img = Image.open(file_path)\n",
      "        pil_img = pil_img.crop(\n",
      "            (w_slice.start, h_slice.start, w_slice.stop, h_slice.stop)\n",
      "        )\n",
      "        if resize is not None:\n",
      "            pil_img = pil_img.resize((w, h))\n",
      "        face = np.asarray(pil_img, dtype=np.float32)\n",
      "\n",
      "        if face.ndim == 0:\n",
      "            raise RuntimeError(\n",
      "                \"Failed to read the image file %s, \"\n",
      "                \"Please make sure that libjpeg is installed\" % file_path\n",
      "            )\n",
      "\n",
      "        face /= 255.0  # scale uint8 coded colors to the [0.0, 1.0] floats\n",
      "        if not color:\n",
      "            # average the color channels to compute a gray levels\n",
      "            # representation\n",
      "            face = face.mean(axis=2)\n",
      "\n",
      "        faces[i, ...] = face\n",
      "\n",
      "    return faces\n",
      "```\n",
      "\n",
      "Now the function should properly crop images according to the `slice_` parameter."
     ]
    }
   ],
   "source": [
    "with open('Data/lfw.py', 'r') as input_file:\n",
    "    lines = input_file.read()\n",
    "    content = 'Find the bug that prevents the _load_imgs function from properly ' \\\n",
    "              'cropping images as specified by the slice_ parameter:\\n' + lines\n",
    "\n",
    "    messages = [{ 'role': 'user', 'content' : content }]\n",
    "    \n",
    "chunks = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see if ChatGPT can generate a Flask Web site that supports user interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I cannot generate a Flask Web site. However, I can provide you with a sample code that you can use as a starting point to build your Flask Web site.\n",
      "\n",
      "Here's the sample code:\n",
      "\n",
      "```python\n",
      "from flask import Flask, render_template, request\n",
      "from textblob import TextBlob\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route('/', methods=['GET', 'POST'])\n",
      "def home():\n",
      "    if request.method == 'POST':\n",
      "        comment = request.form['comment']\n",
      "        sentiment_score = get_sentiment_score(comment)\n",
      "        return render_template('result.html', score=sentiment_score)\n",
      "    return render_template('home.html')\n",
      "\n",
      "def get_sentiment_score(comment):\n",
      "    blob = TextBlob(comment)\n",
      "    return round(blob.sentiment.polarity, 2)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(debug=True)\n",
      "```\n",
      "\n",
      "In this code, we import the necessary modules, including Flask and TextBlob. We define a route for the home page ('/') that accepts both GET and POST requests. If the request method is POST, we get the comment from the form and pass it to the `get_sentiment_score` function to calculate the sentiment score. We then render the `result.html` template and pass the score as a parameter. If the request method is GET, we render the `home.html` template.\n",
      "\n",
      "The `get_sentiment_score` function uses TextBlob to calculate the sentiment polarity of the comment and returns a rounded score.\n",
      "\n",
      "Here's the `home.html` template:\n",
      "\n",
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Sentiment Analysis</title>\n",
      "</head>\n",
      "<body>\n",
      "    <h1>Sentiment Analysis</h1>\n",
      "    <form method=\"POST\">\n",
      "        <label for=\"comment\">Enter your comment:</label><br>\n",
      "        <input type=\"text\" id=\"comment\" name=\"comment\"><br>\n",
      "        <button type=\"submit\">Analyze</button>\n",
      "    </form>\n",
      "</body>\n",
      "</html>\n",
      "```\n",
      "\n",
      "This template contains a form with a text field for the comment and a submit button.\n",
      "\n",
      "Here's the `result.html` template:\n",
      "\n",
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Sentiment Analysis Result</title>\n",
      "</head>\n",
      "<body>\n",
      "    <h1>Sentiment Analysis Result</h1>\n",
      "    <p>The sentiment score of your comment is: {{ score }}</p>\n",
      "</body>\n",
      "</html>\n",
      "```\n",
      "\n",
      "This template displays the sentiment score calculated by the `get_sentiment_score` function.\n",
      "\n",
      "You can customize these templates and the Flask app to fit your needs. Don't forget to install the required modules (`flask` and `textblob`) before running the app."
     ]
    }
   ],
   "source": [
    "content = 'Generate a Flask Web site whose home page contains a text field ' \\\n",
    "          'in which the user types a comment. Include a button that, when ' \\\n",
    "          'clicked, analyzes the comment for sentiment and displays a score ' \\\n",
    "          'from 0.0 to 1.0 indicating how positive the comment is.'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the code work? If you're not sure, follow ChatGPT's instructions and see!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
