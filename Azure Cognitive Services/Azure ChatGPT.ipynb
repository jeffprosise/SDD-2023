{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT\n",
    "\n",
    "[ChatGPT](https://openai.com/blog/chatgpt) is one of several sophisticated AI models built by [OpenAI](https://openai.com/), a San Francisco-based company whose mission is to \"ensure that artificial general intelligence benefits all of humanity.\" ChatGPT is a Large Language Model (LLM) built on GPT-3.5, which boasts 175 billion parameters and ranks as the largest neural network ever built. ChatGPT can generate human-like prose by responding to instructions written in the [Chat Markup Language](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt), or ChatML for short. Here are few examples demonstrating how to leverage ChatGPT using its REST API.\n",
    "\n",
    "![](Images/chatgpt.jpg)\n",
    "\n",
    "Before running this notebook, deploy an instance of ChatGPT in Azure. Then create environment variables named `API_KEY` and `API_ENDPOINT` and set them equal to the OpenAI key and endpoint obtained from the Azure portal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai\n",
    "\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-03-15-preview'\n",
    "openai.api_base = os.environ['API_ENDPOINT'] # Obtained from Azure portal\n",
    "openai.api_key = os.environ['API_KEY'] # Obtained from Azure portal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask ChatGPT to write a paragraph of text describing molecular biology in the style of Dr. Seuss. Run this cell several times and you'll get a different result each time. Set `temperature` to 0.0, however, and the results will be the same most of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the depths of the cell, so small and so fine, \n",
      "Lived tiny molecules, all intertwined. \n",
      "From DNA to RNA, they danced and they swirled, \n",
      "Building proteins that kept the cell world unfurled. \n",
      "\n",
      "The ribosomes were the bosses, they ran the show, \n",
      "Reading the RNA code, producing proteins, don't you know. \n",
      "They made enzymes, and antibodies, and all sorts of things, \n",
      "From hormones to hair, and even those bright butterfly wings. \n",
      "\n",
      "In the nucleus, the DNA was stored, \n",
      "A spiral ladder, twisted and adored. \n",
      "It held the instructions for life's every whim, \n",
      "Coding for every trait, right down to the skin. \n",
      "\n",
      "And when the cell divided, oh boy, what a sight! \n",
      "The DNA replicated, left and right. \n",
      "Each new cell got a copy, just like the old, \n",
      "A genetic treasure trove, so precious and bold. \n",
      "\n",
      "So here's to molecular biology, so complex and grand, \n",
      "Creating all life, from the smallest grain of sand. \n",
      "It's a wondrous world, so full of surprise, \n",
      "All thanks to those little molecules, so clever and wise.\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    'role': 'user',\n",
    "    'content': 'Describe molecular biology in the style of Dr. Seuss'\n",
    "}]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can richen the UI experience by streaming the response. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the cell, oh what a scene\n",
      "Tiny molecules, oh so keen\n",
      "\n",
      "DNA is the star of this show\n",
      "Instructions for life, it does bestow\n",
      "\n",
      "RNA is a messenger, it delivers the plan\n",
      "From the nucleus to the ribosome clan\n",
      "\n",
      "Proteins are the workers, they do the deed\n",
      "Building, breaking, and helping to feed\n",
      "\n",
      "Enzymes are special, oh so grand\n",
      "They speed up reactions, they're in high demand\n",
      "\n",
      "Lipids and carbs, they have a place\n",
      "Supporting the membrane, giving energy with grace\n",
      "\n",
      "In the world of cells, molecular biology is key\n",
      "It's the language of life, can you hear the melody?"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    'role': 'user',\n",
    "    'content': 'Describe molecular biology in the style of Dr. Seuss'\n",
    "}]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messages transmitted to ChatGPT use the Chat Markup Language. ChatML exists so that ChatGPT can be given instructions and so that context can be preserved across calls. To demonstrate, ask ChatGPT what its name is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I don't have a name. You can call me OpenAI. How can I assist you?\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    'role': 'user',\n",
    "    'content': 'My name is Jeff. What\\'s your name?' \\\n",
    "}]\n",
    " \n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jeff, I don't have a real name, you can call me Sam. I'm a chatbot designed to help you with any questions or tasks you might have.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a friendly chatbot named Sam'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'My name is Jeff. What\\'s your name?'\n",
    "    }\n",
    "]\n",
    " \n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can be as specific as you’d like with `system` messages, even saying \"If you don’t know the answer to a question, say I don’t know.\" You can also prescribe a persona. Replace \"friendly\" with \"sarcastic\" in the message from system and run the code again. The response may be \"Oh, hi Jeff, I’m Sam. You can call me whatever you'd like, but don’t call me late for dinner.\" Run the code several times and there’s no end to the colorful responses you’ll receive.\n",
    "\n",
    "ChatML's greatest power lies in persisting context from one call to the next. As an example, try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jeff, as mentioned earlier, my name is Sam. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a friendly chatbot named Sam'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'My name is Jeff. What\\'s your name?'\n",
    "    }\n",
    "]\n",
    " \n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then follow up immediately with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but as an AI language model, I don't have access to your personal information such as your name. Can you please tell me how may I assist you?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a friendly chatbot named Sam'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What is my name?'\n",
    "    }\n",
    "]\n",
    " \n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT will respond with something along the lines of \"I'm sorry, but I don’t have access to that information.\" But now try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Jeff, as you mentioned earlier.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a friendly chatbot named Sam'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'My name is Jeff. What\\'s your name?'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': 'Hello Jeff, my name is Sam. Nice to meet you!'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What is my name?'\n",
    "    }\n",
    "]\n",
    " \n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get it? Calls to ChatGPT are stateless. If you give ChatGPT your name in one call and ask it to repeat your name in the next call, ChatGPT has no clue. But with ChatML, you can provide past ChatGPT responses as context for the current call. You could easily build a conversational bot simply by repeating the last few prompts and responses in each call to ChatGPT. The further back you go, the longer the chatbot’s \"memory\" will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "ChatGPT doesn't work with words; it works with *tokens*. Tokenization plays an important role in Natural Language Processing. Neural networks can’t process text, at least not directly; they only process numbers. Tokenization converts words into numbers that a deep-learning model can understand. When ChatGPT generates a response by predicting a series of tokens, the tokenization process is reversed to convert the tokens into human-readable text.\n",
    "\n",
    "ChatGPT uses a form of tokenization called [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (BPE), which was developed in the 1990s as a mechanism for compressing text. Today, it is widely used in the NLP space. Here’s how ChatGPT BPE-tokenizes the phrase \"fourscore and seven years ago:\"\n",
    "\n",
    "![](Images/bpe.png)\n",
    "\n",
    "As a rule of thumb, 3 words on average translate to about 4 BPE tokens. That’s important because ChatGPT limits the number of tokens in each API call. The maximum token count is controlled by a parameter named `max_tokens`. For ChatGPT, the default is 2,048 tokens or about 1,500 words, and the upper limit is 4,096. (GPT-4 expands the maximum token count to 32,768.) This limit applies to the combined length of the input and output in each API call. If the number of tokens exceeds `max_tokens`, then either the call will fail or the response will be truncated.\n",
    "\n",
    "You can compute the number of tokens generated from a text sample with help from a Python package named [`tiktoken`](https://pypi.org/project/tiktoken/0.3.0/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 tokens\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    " \n",
    "text = 'Jeff loves to build and fly model jets. He built his first ' \\\n",
    "       'jet, a BVM BobCat, in 2007. After that, he built a BVM Bandit, ' \\\n",
    "       'a Skymaster F-16, and a Skymaster F-5. The latter two are 1/6th' \\\n",
    "       'scale models of actual fighter jets. Top speed is around 200 MPH.'\n",
    " \n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "num_tokens = len(encoding.encode(text))\n",
    "print(f'{num_tokens} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can estimate the token count for an entire `messages` array with the following code, which was adapted comments and all from the [ChatGPT documentation](https://platform.openai.com/docs/guides/chat/introduction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 tokens\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 0\n",
    " \n",
    "for message in messages:\n",
    "    num_tokens += 4 # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "    for key, value in message.items():\n",
    "        num_tokens += len(encoding.encode(value))\n",
    "        if key == 'name':  # if there's a name, the role is omitted\n",
    "            num_tokens += -1 # role is always required and always 1 token\n",
    "             \n",
    "num_tokens += 2 # every reply is primed with <im_start>assistant\n",
    "print(f'{num_tokens} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of reasons to be aware of the token count in each call. First, you’re charged by the token for input and output. The price at the time of this writing was 0.2 cents per 1,000 tokens, which equates to 500,000 tokens – or roughly 375,000 words – per U.S. dollar. The larger the `messages` array and the longer the response, the more you pay. Second, when using the messages array to provide context from previous calls, you have a finite amount of space to work with. It's common practice to pick a number – say, 5 or 10 – and limit the context from previous calls to that number of messages, or to programmatically compute the number of tokens that a conversation comprises and include as many messages as `max_tokens` will allow while leaving room for the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text\n",
    "\n",
    "ChatGPT can perform many NLP tasks such as sentiment analysis and neural machine translation (NMT) without further training. Here's an example that translates text from English to French. It's a good idea to set `temperature` to 0 here since you generally want translations to be accurate and repeatable rather than creative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeff adore construire et piloter des jets modèles réduits. Il a construit son premier jet, un BVM BobCat, en 2007. Après cela, il a construit un BVM Bandit, un Skymaster F-16 et un Skymaster F-5. Ces deux derniers sont des modèles à l'échelle 1/6 de véritables avions de chasse. La vitesse maximale est d'environ 200 MPH.\n"
     ]
    }
   ],
   "source": [
    "content =  f'Translate the following text from English to French: {text}'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT understands dozens of languages. It even knows Klingon. Let's see how the same paragraph translates to Klingon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeff vIghro' Dujmey je jets modelmey je 'oH. 'Ivqu' 'oH jet Dujmey, BVM BobCat, 2007. Hoch, 'oH Dujmey BVM Bandit, Skymaster F-16, 'ej Skymaster F-5 Dujmey. cha'logh cha'logh jet fighter Dujmey 1/6th scale. chenmoHwI' 200 MPH.\n"
     ]
    }
   ],
   "source": [
    "content = f'Translate the following text from English to Klingon: {text}'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the practical uses for ChatGPT is editing existing text to make it more compelling. Here's an example that generates a marketing blurb for a webinar on AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discover the fascinating world of AI and how it's revolutionizing the way businesses operate! Dive deep into the inner workings of this cutting-edge technology and unleash the unlimited potential it has to offer. From boosting efficiency to elevating customer experience, AI is the driving force behind the next generation of business success. Join us on this thrilling journey of exploration and seize the opportunity to master the skills businesses demand today!\n"
     ]
    }
   ],
   "source": [
    "content = 'Make the following marketing text more engaging: ' \\\n",
    "          'Learn how AI works and how it\\'s used in business'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples demonstrate how to use ChatGPT for sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "content = 'Indicate whether the following review\\'s sentiment is positive or ' \\\n",
    "          'negative: Great food and excellent service'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "content = 'Indicate whether the following review\\'s sentiment is positive or ' \\\n",
    "          'negative: Long lines and poor customer service'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is a text-classification task. ChatGPT can classify text in other ways, too. The next two examples demonstrate how it could be used as the basis for a spam filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not spam.\n"
     ]
    }
   ],
   "source": [
    "content = 'Indicate whether the following email is spam or not spam: ' \\\n",
    "          'Please plan to attend the code review at 2:00 p.m. this afternoon'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "content = 'Indicate whether the following email is spam or not spam: ' \\\n",
    "          'Order prescription meds online and save $$$'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT was trained on a massive body of data that includes [Common Crawl](https://commoncrawl.org/), a pair of book databases known as Books1 and Books2, [WebText2](https://www.eleuther.ai/projects/owt2/), and [Wikipedia](https://www.wikipedia.org/), so it's knowledge is vast. It can generally answer questions as long as the answers are somewhere on the Internet. Its answers aren't always correct because (surprise!) there's a lot of misinformation on the Internet. Here's a question that's pertinent to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The deep learning transformer was introduced in the academic paper \"Attention Is All You Need\" by Ashish Vaswani et al. It was presented at the 2017 Conference on Neural Information Processing Systems (NIPS) and can be found on the conference website at https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\n"
     ]
    }
   ],
   "source": [
    "content = 'In which academic paper was the deep learning transformer ' \\\n",
    "          'introduced, and where can I find it?'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT can also answer questions from context that you provide. Here's an example that involves my hobby. Observe that ChatGPT seems to have the ability to count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeff has built four jets.\n"
     ]
    }
   ],
   "source": [
    "question = 'How many jets has Jeff built?'\n",
    "\n",
    "content = f'Answer the following question using the provided context, and if the ' \\\n",
    "          f'answer is not contained within the context, say \"I don\\'t know.\"\\n\\n' \\\n",
    "          f'Context: {text}\\n\\n' \\\n",
    "          f'Q: {question}\\n\\n' \\\n",
    "          f'A: '\n",
    "\n",
    "messages = [{ 'role': 'user', 'content': content }]\n",
    "    \n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT is frequently used to ingest call transcripts and call notes and answer questions for statements of work (SOWs) and scoping documents. Here's an example involving the transcript of a recent [PBS interview](https://www.pbs.org/thinktank/transcript1292.html) with Elon Musk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk is affiliated with several companies, including Tesla Motors, which makes electric cars, and SpaceX, which develops markets for taking satellites and people to orbit and beyond. He was also involved with PayPal, the leading internet payment mechanism."
     ]
    }
   ],
   "source": [
    "with open('Data/musk.txt', 'r') as input_file:\n",
    "    text = input_file.read()\n",
    "    \n",
    "    question = 'What companies is Elon Musk affiliated with, and what do those companies do?'\n",
    "\n",
    "    content = f'Answer the following question using the provided context, and if the ' \\\n",
    "              f'answer is not contained within the context, say \"I don\\'t know.\"\\n\\n' \\\n",
    "              f'Context: {text}\\n\\n' \\\n",
    "              f'Q: {question}\\n\\n' \\\n",
    "              f'A: '\n",
    "\n",
    "    messages = [{ 'role': 'user', 'content': content }]\n",
    "    \n",
    "    chunks = openai.ChatCompletion.create(\n",
    "        engine='my-chatgpt',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for chunk in chunks:\n",
    "        content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "        if content is not None:\n",
    "            print(content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to read the transcript and submit questions of your own to see how ChatGPT responds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with code\n",
    "\n",
    "In addition to being trained on massive volumes of text comprising approximately 500 billion words, ChatGPT was trained with billions of lines of code. It can generate code, comment code, find bugs in code, and more, and it supports dozens of programming languages. Let's use a few examples to see what it's capable of. Start by using ChatGPT to implement a bubble sort in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I can certainly provide you with the code you requested. Here's the code in Python for sorting an array of numbers using bubble sort:\n",
      "\n",
      "```python\n",
      "def bubble_sort(arr):\n",
      "    n = len(arr)\n",
      "    \n",
      "    for i in range(n):\n",
      "        for j in range(0, n-i-1):\n",
      "            if arr[j] > arr[j+1]:\n",
      "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "                \n",
      "    return arr\n",
      "```\n",
      "\n",
      "The `bubble_sort()` function takes an array of numbers as input and iterates over it, comparing adjacent elements and swapping them if they are in the wrong order. This process repeats until the array is fully sorted. The sorted array is then returned.\n",
      "\n",
      "You can use this function to sort an array of numbers as follows:\n",
      "\n",
      "```python\n",
      "my_arr = [3, 7, 1, 20, 5]\n",
      "sorted_arr = bubble_sort(my_arr)\n",
      "print(sorted_arr)\n",
      "```\n",
      "\n",
      "This will output: `[1, 3, 5, 7, 20]`"
     ]
    }
   ],
   "source": [
    "content = 'Create a Python function that accepts an array of numbers as ' \\\n",
    "          'input, bubble sorts the numbers, and returns a sorted array'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ask ChatGPT to explain the code that it just generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code is implementing the bubble sort algorithm to sort the elements of a given array in ascending order. The larger values in the array \"bubble up\" to the end of the array in each iteration. \n",
      "\n",
      "The function takes an array \"arr\" as input and first initializes a variable \"n\" to the length of the array. It then iterates \"n\" times through the outer loop, where each iteration corresponds to placing the ith largest element in its sorted position.\n",
      "\n",
      "Within the outer loop, the code implements an inner loop that iterates up to the (n-i-1)th index of the array. This is because the elements from i+1 to n-1 have already bubbled up to their sorted positions, so there is no need to compare them again.\n",
      "\n",
      "Within the inner loop, the code compares each element with its neighbor to the right. If the left element is greater than the right element, the two elements are swapped. This causes the larger element to \"bubble up\" towards the end of the array. \n",
      "\n",
      "After the inner loop finishes, the ith largest element has been placed in its sorted position at the end of the array. The outer loop then moves on to the next largest element until the entire array is sorted. Finally, the sorted array is returned as output."
     ]
    }
   ],
   "source": [
    "content = 'Explain what the following code does:\\n' \\\n",
    "          'def bubble_sort(arr):\\n' \\\n",
    "          '    n = len(arr)\\n' \\\n",
    "          '    for i in range(n):\\n' \\\n",
    "          '        for j in range(0, n-i-1):\\n' \\\n",
    "          '            if arr[j] > arr[j+1]:\\n' \\\n",
    "          '                arr[j], arr[j+1] = arr[j+1], arr[j]\\n' \\\n",
    "          '    return arr'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ChatGPT to convert a Python function to FORTRAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subroutine bubble_sort(arr,n)\n",
      "    integer :: n, i, j\n",
      "    real :: arr(n)\n",
      "    do i=1,n\n",
      "        do j=1,n-i\n",
      "            if (arr(j) > arr(j+1)) then\n",
      "                temp = arr(j)\n",
      "                arr(j) = arr(j+1)\n",
      "                arr(j+1) = temp\n",
      "            end if\n",
      "        end do\n",
      "    end do\n",
      "end subroutine"
     ]
    }
   ],
   "source": [
    "content = 'Convert the following Python function into a FORTRAN function:\\n' \\\n",
    "          'def bubble_sort(arr):\\n' \\\n",
    "          '    n = len(arr)\\n' \\\n",
    "          '    for i in range(n):\\n' \\\n",
    "          '        for j in range(0, n-i-1):\\n' \\\n",
    "          '            if arr[j] > arr[j+1]:\\n' \\\n",
    "          '                arr[j], arr[j+1] = arr[j+1], arr[j]\\n' \\\n",
    "          '    return arr'\n",
    "\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ChatGPT to add comments to a block of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define a function that sorts an array using bubble sort algorithm\n",
      "def bubble_sort(arr):\n",
      "    # Get the length of the array\n",
      "    n = len(arr)\n",
      "    # Loop through all elements in the array\n",
      "    for i in range(n):\n",
      "        # Loop through all elements except the last i elements\n",
      "        for j in range(0, n-i-1):\n",
      "            # Swap adjacent elements if the first one is greater than the second one\n",
      "            if arr[j] > arr[j+1]:\n",
      "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "    # Return the sorted array\n",
      "    return arr"
     ]
    }
   ],
   "source": [
    "content = 'Add inline comments to the following code:\\n' \\\n",
    "          'def bubble_sort(arr):\\n' \\\n",
    "          '    n = len(arr)\\n' \\\n",
    "          '    for i in range(n):\\n' \\\n",
    "          '        for j in range(0, n-i-1):\\n' \\\n",
    "          '            if arr[j] > arr[j+1]:\\n' \\\n",
    "          '                arr[j], arr[j+1] = arr[j+1], arr[j]\\n' \\\n",
    "          '    return arr'\n",
    "\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to write a tool to add comments to uncommented source-code files. The next example loads **app.py**, adds comments to it, and saves the results in **app-commented.py**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "with open('Data/app.py', 'r') as input_file:\n",
    "    lines = input_file.read()\n",
    "    content = 'Add inline comments to the following code:\\n' + lines\n",
    "\n",
    "    messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    with open('Data/app-commented.py', 'w') as output_file:\n",
    "        output_file.write(response.choices[0].message.content)\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works with C# source code as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "with open('Data/Program.cs', 'r') as input_file:\n",
    "    lines = input_file.read()\n",
    "    content = 'Add inline comments to the following code:\\n' + lines\n",
    "\n",
    "    messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine='my-chatgpt',\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    with open('Data/Program-commented.cs', 'w') as output_file:\n",
    "        output_file.write(response.choices[0].message.content)\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaking of C#: Can ChatGPT rewrite a block of C# code that manually iterates over a `List` to use LINQ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var picks = stocks.Where(stock => stock.Close > stock.Open).ToList();\n",
      "picks.ForEach(pick => Console.WriteLine($\"{pick.Symbol}: {pick.Open:c} -> {pick.Close:c}\"));\n"
     ]
    }
   ],
   "source": [
    "content = 'Rewrite the following C# code to use LINQ:\\n' \\\n",
    "          'var picks = new List<DailyStock>();\\n' \\\n",
    "          'foreach (var stock in stocks)\\n' \\\n",
    "          '{\\n' \\\n",
    "          '    if (stock.Close > stock.Open)\\n' \\\n",
    "          '    {\\n' \\\n",
    "          '        picks.Add(stock);\\n' \\\n",
    "          '    }\\n' \\\n",
    "          '}\\n' \\\n",
    "          'foreach(var pick in picks)\\n' \\\n",
    "          '{\\n' \\\n",
    "          '    Console.WriteLine($\"{pick.Symbol}: {pick.Open:c} -> {pick.Close:c}\");\\n' \\\n",
    "          '}\\n'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ChatGPT to generate a non-trivial SQL query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Department.name\n",
      "FROM Department\n",
      "INNER JOIN Employee ON Employee.department_id = Department.id\n",
      "INNER JOIN Salary_Payments ON Salary_Payments.employee_id = Employee.id\n",
      "WHERE Salary_Payments.date >= DATEADD(month, -3, GETDATE())\n",
      "GROUP BY Department.name\n",
      "HAVING COUNT(Employee.id) >= 10;"
     ]
    }
   ],
   "source": [
    "content = 'Generate a SQL query to list the names of all departments ' \\\n",
    "          'that have employed 10 or more people in the last 3 months. ' \\\n",
    "          'The query targets a table with the following schema:\\n' \\\n",
    "          'Employee(id, name, department_id)\\n' \\\n",
    "          'Department(id, name, address)\\n' \\\n",
    "          'Salary_Payments(id, employee_id, amount, date)\\n'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT is cognizant of popular programming libraries and frameworks, too. Here, it relies on one of them to generate a function that performs sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I don't have access to sentiment analysis libraries. However, here's an example of how you can use the TextBlob library to perform sentiment analysis in Python:\n",
      "\n",
      "```python\n",
      "from textblob import TextBlob\n",
      "\n",
      "def analyze_sentiment(text):\n",
      "    blob = TextBlob(text)\n",
      "    sentiment = blob.sentiment.polarity\n",
      "    return (sentiment + 1) / 2  # Normalize to range [0, 1]\n",
      "\n",
      "# Example usage\n",
      "text = \"I love this product! It's amazing!\"\n",
      "sentiment = analyze_sentiment(text)\n",
      "print(sentiment)  # Output: 0.95\n",
      "```\n",
      "\n",
      "In this example, we use the `TextBlob` library to create a `TextBlob` object from the input text. We then use the `sentiment` property of the `TextBlob` object to get the polarity of the sentiment, which is a value between -1.0 (very negative) and 1.0 (very positive). We then normalize this value to the range [0, 1] by adding 1 and dividing by 2. Finally, we return the normalized sentiment value."
     ]
    }
   ],
   "source": [
    "content = 'Generate a Python function that accepts a string as input and ' \\\n",
    "          'analyzes the string for sentiment. The function returns a value ' \\\n",
    "          'from 0.0 to 1.0, where 0.0 means the sentiment is very negative ' \\\n",
    "          'and 1.0 means it is very positive.'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the more remarkable aspects of ChatGPT is its ability to find bugs in code. Just after my book went to the printer in 2022, I discovered a bug in the latest version of Scikit-learn that prevented some of my samples from working properly. I spent a couple of hours in the source code and found the bug. I filed a [bug report](https://github.com/scikit-learn/scikit-learn/issues/24942) and the Scikit team confirmed the bug and promised to fix it in the next version. I had to scramble to rewrite some of the code samples in my book to work around the bug and get the changes to the printer before the presses started rolling. Let's see if ChatGPT can find the bug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bug is that the cropped portion of the image is not saved back to the pil_img object, so it is not actually used to create the numpy array. To fix this, we need to assign the result of the crop function back to pil_img:\n",
      "\n",
      "```\n",
      "pil_img = Image.open(file_path)\n",
      "pil_img = pil_img.crop(\n",
      "            (w_slice.start, h_slice.start, w_slice.stop, h_slice.stop)\n",
      "        )\n",
      "if resize is not None:\n",
      "    pil_img = pil_img.resize((w, h))\n",
      "face = np.asarray(pil_img, dtype=np.float32)\n",
      "```"
     ]
    }
   ],
   "source": [
    "with open('Data/lfw.py', 'r') as input_file:\n",
    "    lines = input_file.read()\n",
    "    content = 'Find the bug that prevents the _load_imgs function from properly ' \\\n",
    "              'cropping images as specified by the slice_ parameter:\\n' + lines\n",
    "\n",
    "    messages = [{ 'role': 'user', 'content' : content }]\n",
    "    \n",
    "chunks = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see if ChatGPT can generate a Flask Web site that supports user interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I cannot generate a Flask Web site. However, I can provide you with a sample code that you can use as a starting point to build your Flask Web site.\n",
      "\n",
      "Here's the sample code:\n",
      "\n",
      "```python\n",
      "from flask import Flask, render_template, request\n",
      "from textblob import TextBlob\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route('/', methods=['GET', 'POST'])\n",
      "def home():\n",
      "    if request.method == 'POST':\n",
      "        comment = request.form['comment']\n",
      "        sentiment_score = get_sentiment_score(comment)\n",
      "        return render_template('result.html', score=sentiment_score)\n",
      "    return render_template('home.html')\n",
      "\n",
      "def get_sentiment_score(comment):\n",
      "    blob = TextBlob(comment)\n",
      "    return round(blob.sentiment.polarity, 2)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(debug=True)\n",
      "```\n",
      "\n",
      "In this code, we import the necessary modules, including Flask and TextBlob. We define a route for the home page ('/') that accepts both GET and POST requests. If the request method is POST, we get the comment from the form and pass it to the `get_sentiment_score` function, which uses TextBlob to analyze the sentiment of the comment and return a score between -1.0 and 1.0. We then render a template called 'result.html' and pass the score as a parameter. If the request method is GET, we simply render the 'home.html' template.\n",
      "\n",
      "Here's the 'home.html' template:\n",
      "\n",
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Sentiment Analysis</title>\n",
      "</head>\n",
      "<body>\n",
      "    <h1>Sentiment Analysis</h1>\n",
      "    <form method=\"POST\">\n",
      "        <label for=\"comment\">Enter your comment:</label>\n",
      "        <input type=\"text\" name=\"comment\" id=\"comment\">\n",
      "        <button type=\"submit\">Analyze</button>\n",
      "    </form>\n",
      "</body>\n",
      "</html>\n",
      "```\n",
      "\n",
      "This template contains a form with a text field for the user to enter their comment and a button to submit the form.\n",
      "\n",
      "Here's the 'result.html' template:\n",
      "\n",
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Sentiment Analysis Result</title>\n",
      "</head>\n",
      "<body>\n",
      "    <h1>Sentiment Analysis Result</h1>\n",
      "    <p>The sentiment score of your comment is: {{ score }}</p>\n",
      "</body>\n",
      "</html>\n",
      "```\n",
      "\n",
      "This template simply displays the sentiment score returned by the `get_sentiment_score` function.\n",
      "\n",
      "You can run this Flask app by saving the code in a file called 'app.py' and running the command `python app.py` in your terminal. Then, open your web browser and go to http://localhost:5000 to see the home page."
     ]
    }
   ],
   "source": [
    "content = 'Generate a Flask Web site whose home page contains a text field ' \\\n",
    "          'in which the user types a comment. Include a button that, when ' \\\n",
    "          'clicked, analyzes the comment for sentiment and displays a score ' \\\n",
    "          'from 0.0 to 1.0 indicating how positive the comment is.'\n",
    "\n",
    "messages = [{ 'role': 'user', 'content' : content }]\n",
    "\n",
    "chunks = openai.ChatCompletion.create(\n",
    "    engine='my-chatgpt',\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    content = chunk['choices'][0].get('delta', {}).get('content')\n",
    "    if content is not None:\n",
    "        print(content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the code work? If you're not sure, follow ChatGPT's instructions and see!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
