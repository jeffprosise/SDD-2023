{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Services\n",
    "\n",
    "Azure Cognitive Services includes a pair of services that are backed by deep-learning models trained to perform natural language processing (NLP). The [Language](https://azure.microsoft.com/en-us/services/cognitive-services/language-service/#overview) service provides APIs for sentiment analysis, named-entity recognition, question answering, key-phrase extraction, language understanding, and more. Among its many uses is building chatbots that respond intelligently to queries regarding a company’s product or service offerings. There’s also a [Translator](https://azure.microsoft.com/en-us/services/cognitive-services/translator/) service that translates text between more than 100 languages and dialects. Volkswagen uses it to [translate onscreen instructions in cars](https://customers.microsoft.com/en-us/story/779468-volkswagen-azure-automotive-en) to match the language in the owner’s manual and to ensure the quality of the documentation that they produce in more than 40 languages.\n",
    "\n",
    "The [`TextAnalyticsClient`](https://docs.microsoft.com/en-us/python/api/azure-ai-textanalytics/azure.ai.textanalytics.textanalyticsclient?view=azure-python) class in the [Python text-analytics SDK](https://pypi.org/project/azure-ai-textanalytics/) provides a convenient interface to many of the Language service’s features. Here’s an example that analyzes a text sample for sentiment. Replace `KEY` with a subscription key for the Language service and `ENDPOINT` with an endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0.94, 'neutral': 0.05, 'negative': 0.01}\n"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "client = TextAnalyticsClient(ENDPOINT, AzureKeyCredential(KEY))\n",
    "input = [{ 'id': '1000', 'text': 'Programming is fun, but the hours are long' }]\n",
    "response = client.analyze_sentiment(input)\n",
    "\n",
    "for result in response:\n",
    "    print(result.confidence_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s another example that applies sentiment analysis to multiple text samples with one call to the Language service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programming is fun, but the hours are long => 0.94\n",
      "Great food and excellent service => 1.0\n",
      "The product worked as advertised but is overpriced => 0.0\n",
      "Moving to the cloud was the best decision we ever made => 1.0\n",
      "Programming is so fun I'd do it for free. Don't tell my boss! => 1.0\n"
     ]
    }
   ],
   "source": [
    "client = TextAnalyticsClient(ENDPOINT, AzureKeyCredential(KEY))\n",
    "\n",
    "input = [\n",
    "    { 'id': '1000', 'text': 'Programming is fun, but the hours are long' },\n",
    "    { 'id': '1001', 'text': 'Great food and excellent service' },\n",
    "    { 'id': '1002', 'text': 'The product worked as advertised but is overpriced' },\n",
    "    { 'id': '1003', 'text': 'Moving to the cloud was the best decision we ever made' },\n",
    "    { 'id': '1004', 'text': 'Programming is so fun I\\'d do it for free. Don\\'t tell my boss!' }\n",
    "]\n",
    "\n",
    "response = client.analyze_sentiment(input)\n",
    "\n",
    "for result in response:\n",
    "    text = ''.join([x.text for x in result.sentences])\n",
    "    print(f'{text} => {result.confidence_scores.positive}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is one of several operations supported by the `TextAnalyticsClient` class. Another is named-entity recognition. Suppose you’re building a system that sorts and prioritizes support tickets received by your company’s help desk. The [`recognize_entities`](https://docs.microsoft.com/en-us/python/api/azure-ai-textanalytics/azure.ai.textanalytics.textanalyticsclient?view=azure-python#azure-ai-textanalytics-textanalyticsclient-recognize-entities) method recognizes entities such as people, places, organizations, dates and times, and quantities in input text. It extracts them and reveals the entity types as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printer (Product)\n",
      "IT (Skill)\n",
      "office (Location)\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    'My printer isn\\'t working. Can someone from IT come to my office and have a look?'\n",
    "]\n",
    "\n",
    "results = client.recognize_entities(documents)\n",
    "\n",
    "for result in results:\n",
    "    for entity in result.entities:\n",
    "        print(f'{entity.text} ({entity.category})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related method named [`recognize_pii_entities`](https://docs.microsoft.com/en-us/python/api/azure-ai-textanalytics/azure.ai.textanalytics.textanalyticsclient?view=azure-python#azure-ai-textanalytics-textanalyticsclient-recognize-pii-entities) extracts personally identifiable information (PII) entities such as bank-account info, social-security numbers, and credit-card numbers from input text, while the [`extract_key_phrases`](https://docs.microsoft.com/en-us/python/api/azure-ai-textanalytics/azure.ai.textanalytics.textanalyticsclient?view=azure-python#azure-ai-textanalytics-textanalyticsclient-extract-key-phrases) method extracts key phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing\n",
      "language translation\n",
      "text classification\n",
      "keyword extraction\n",
      "question answering\n",
      "NLP\n",
      "variety\n",
      "activities\n",
      "recognition\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    'Natural Language Processing, or NLP, encompasses a variety of activities ' \\\n",
    "    'including text classification, keyword extraction, named-entity recognition, ' \\\n",
    "    'question answering, and language translation.'\n",
    "]\n",
    "\n",
    "results = client.extract_key_phrases(documents)\n",
    "\n",
    "for result in results:\n",
    "    for phrase in result.key_phrases:\n",
    "        print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextAnalyticsClient` provides a wrapper around text-analytics APIs. Other Python SDKs unlock additional features of the Language service. For example, the [question-answering SDK](https://pypi.org/project/azure-ai-language-questionanswering/) provides APIs for answering questions from manuals, FAQs, blog posts, and other documents that you provide. Here's an example that uses the SDK's `QuestionAnsweringClient` class to answer a question from a provided context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thirty Years (88.0%)\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.language.questionanswering import models as qna\n",
    "from azure.ai.language.questionanswering import QuestionAnsweringClient\n",
    "\n",
    "client = QuestionAnsweringClient(ENDPOINT, AzureKeyCredential(KEY))\n",
    "\n",
    "question = 'What is the minimum age required to serve as a United State Senator?'\n",
    "\n",
    "context = 'No Person shall be a Senator who shall not have attained to the Age of ' \\\n",
    "          'thirty Years, and been nine Years a Citizen of the United States, and ' \\\n",
    "          'who shall not, when elected, be an Inhabitant of that State for which ' \\\n",
    "          'he shall be chosen.'\n",
    "\n",
    "input = qna.AnswersFromTextOptions(question=question, text_documents=[context])\n",
    "results = client.get_answers_from_text(input)\n",
    "\n",
    "answer = results.answers[0].short_answer\n",
    "print(f'{answer.text} ({answer.confidence:.1%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, refer to the article entitled [Azure Cognitive Language Services Question Answering client library for Python](https://docs.microsoft.com/en-us/python/api/overview/azure/ai-language-questionanswering-readme?view=azure-python).\n",
    "\n",
    "The Translator service uses state-of-the-art neural machine translation to translate text between dozens of languages. It can also identify written languages. Suppose your objective is to translate into English questions written in other languages and submitted through your company’s Web site. First you need to determine whether the source language is English. If it’s not, you want to translate it so you can respond to the customer’s request.\n",
    "\n",
    "The code below analyzes a text sample and shows the language it’s written in. Microsoft doesn’t currently offer a Python SDK for the Translator service, but you can use Python’s [`requests`](https://pypi.org/project/requests/) package to simplify calls. To demonstrate, create a Translator resource in the Azure Portal and grab the subscription key (the one labeled “Text Translation”), endpoint, and region. Then run the following code, replacing `KEY` and `ENDPOINT` with the key and endpoint and `REGION` with the Azure region you selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "input = [{ 'text': 'Quand votre nouveau livre sera-t-il disponible?' }]\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': KEY,\n",
    "    'Ocp-Apim-Subscription-Region': REGION,\n",
    "    'Content-type': 'application/json'\n",
    "}\n",
    "\n",
    "uri = ENDPOINT + 'detect?api-version=3.0&to=en'\n",
    "response = requests.post(uri, headers=headers, json=input)\n",
    "results = response.json()\n",
    "\n",
    "print(results[0]['language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you’ve determined the source language isn’t English, you can translate it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When will your new book be available?\n"
     ]
    }
   ],
   "source": [
    "uri = ENDPOINT + 'translate?api-version=3.0&from=fr&to=en'\n",
    "response = requests.post(uri, headers=headers, json=input)\n",
    "results = response.json()\n",
    "\n",
    "print(results[0]['translations'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you obtained an endpoint for the Translator service, did you notice that the Azure Portal offered two endpoints? One for “Text Translation” and another for “Document Translation?” That’s because the Translator service features a second API for translating entire documents, including PDFs. There’s even a [Python SDK](https://pypi.org/project/azure-ai-translation-document/) to help out. The only catch is that documents must first be uploaded to Azure blob storage. For examples showing document-translation API in action, see [Azure Document Translation client library for Python](https://docs.microsoft.com/en-us/python/api/overview/azure/ai-translation-document-readme?view=azure-python). The API is asynchronous and can process batches of documents in one call, so it’s ideal not just for translating individual documents, but for translating large volumes of documents at scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
