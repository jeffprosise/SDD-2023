{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Speech Service\n",
    "\n",
    "One of the more challenging tasks for deep-learning models is processing human speech. Azure Cognitive Services includes a [Speech](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/overview) service that converts text to speech, speech to text, and more. It’s even capable of [captioning recorded videos and live video streams](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/captioning-concepts?pivots=programming-language-python) and filtering out profanity as it does. A Python SDK simplifies the code you write and makes it remarkably easy to incorporate speech into your apps.\n",
    "\n",
    "To demonstrate, install the package named [`azure-cognitiveservices-speech`](https://pypi.org/project/azure-cognitiveservices-speech/) containing the Speech SDK. Use the Azure Portal to create a Cognitive Services Speech resource and make note of the subscription key and service region. Then run the following code after replacing `KEY` with the subscription key and `REGION` with the region you selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices import speech\n",
    "\n",
    "speech_config = speech.SpeechConfig(KEY, REGION)\n",
    "speech_config.speech_recognition_language = 'en-US'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following statements and when prompted, speak into your microphone. This example creates a [`SpeechRecognizer`](https://docs.microsoft.com/en-us/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.speechrecognizer?view=azure-python) object and uses its [`recognize_once_async`](https://docs.microsoft.com/en-us/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.speechrecognizer?view=azure-python#azure-cognitiveservices-speech-speechrecognizer-recognize-once-async) method to convert up to 30 seconds of live audio from your PC’s default microphone into text. Observe that the text doesn’t appear until you’ve finished speaking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone\n",
      "When will your new book be published?\n"
     ]
    }
   ],
   "source": [
    "recognizer = speech.SpeechRecognizer(speech_config)\n",
    "\n",
    "print('Speak into your microphone')\n",
    "result = recognizer.recognize_once_async().get()\n",
    "\n",
    "if result.reason == speech.ResultReason.RecognizedSpeech:\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about converting text to speech? Here’s an example that uses the SDK’s `SpeechSynthesizer` class to vocalize a sentence. The synthesized voice belongs to an  English speaker named Jenny (\"en-US-JennyNeural\"), and it’s one of more than [300 neural voices](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/language-support?tabs=speechtotext#prebuilt-neural-voices) you can choose from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.cognitiveservices.speech.SpeechSynthesisResult at 0x1787c9b42b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_config.speech_synthesis_voice_name = 'en-US-JennyNeural'\n",
    "synthesizer = speech.SpeechSynthesizer(speech_config)\n",
    "synthesizer.speak_text_async('When will your new book be published?').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the “speakers” are multilingual. If you ask a French speaker – for example, \"fr-FR-CelesteNeural\" – to synthesize an English sentence, the vocalization will feature a French accent.\n",
    "\n",
    "You can combine a [`TranslationRecognizer`](https://docs.microsoft.com/en-us/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.translation.translationrecognizer?view=azure-python) object with a `SpeechSynthesizer` object to translate speech in real time. The following example takes spoken English as input and plays it back in French using the voice of a native French speaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone\n"
     ]
    }
   ],
   "source": [
    "speech_config.speech_synthesis_voice_name = 'fr-FR-YvetteNeural'\n",
    "synthesizer = speech.SpeechSynthesizer(speech_config)\n",
    "\n",
    "translation_config = speech.translation.SpeechTranslationConfig(KEY, REGION)\n",
    "translation_config.speech_recognition_language = 'en-US'\n",
    "translation_config.add_target_language('fr')\n",
    "\n",
    "recognizer = speech.translation.TranslationRecognizer(translation_config)\n",
    "\n",
    "print('Speak into your microphone')\n",
    "result = recognizer.recognize_once_async().get()\n",
    "\n",
    "if result.reason == speech.ResultReason.TranslatedSpeech:\n",
    "    text = result.translations['fr']\n",
    "    synthesizer.speak_text_async(text).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These samples use your PC’s default microphone for voice input and default speakers for output. You can specify other sources of input and output by passing an [`AudioConfig`](https://docs.microsoft.com/en-us/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.audio.audioconfig?view=azure-python) object to the methods that create `SpeechRecognizer`, `SpeechSynthesizer`, and `TranslationRecognizer` objects. Among the options this enables is using a file or stream rather than a microphone as the source of input."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
